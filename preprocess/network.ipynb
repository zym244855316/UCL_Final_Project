{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/uclresearchanalysis\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reading config file from location: '\n",
      " '/home/ec2-user/uclresearchanalysis/configuration/env.properties')\n",
      "{'calculate': {'analysis': True,\n",
      "               'friends': True,\n",
      "               'network': True,\n",
      "               'uniquetweets': True,\n",
      "               'uniqueusers': True},\n",
      " 'data': {'dates': ['2017-12-10', '2017-12-11', '2017-12-12'],\n",
      "          'eventname': 'nyc attack',\n",
      "          'phrases': ['nyc%20explosion',\n",
      "                      'nyc%20bombing',\n",
      "                      'nyc%20attack',\n",
      "                      'nyc%20terror',\n",
      "                      'new%20york%20explosion',\n",
      "                      'new%20york%20bombing',\n",
      "                      'new%20york%20attack',\n",
      "                      'new%20york%20terror',\n",
      "                      'manhattan%20explosion',\n",
      "                      'manhattan%20bombing',\n",
      "                      'manhattan%20attack',\n",
      "                      'manhattan%20terror',\n",
      "                      'port%20authority%20explosion',\n",
      "                      'port%20authority%20bombing',\n",
      "                      'port%20authority%20attack',\n",
      "                      'port%20authority%20terror'],\n",
      "          'starttime': 'Dec 11 07:00:00 -0500 2017'},\n",
      " 'path': {'cwd': '/home/ec2-user/uclresearchanalysis/data/nyc',\n",
      "          'ml': '/home/ec2-user/uclresearchanalysis/data/nyc/pickle',\n",
      "          'networkx': {'all': '/home/ec2-user/uclresearchanalysis/data/nyc/pickle/networkx_all.dat',\n",
      "                       'friends': '/home/ec2-user/uclresearchanalysis/data/nyc/pickle/networkx_friends.dat',\n",
      "                       'potential': '/home/ec2-user/uclresearchanalysis/data/nyc/pickle/networkx_potential.dat'},\n",
      "          'newcrawl': '/home/ec2-user/uclresearchanalysis/other/newcrawl.dat',\n",
      "          'pickle': {'friends': '/home/ec2-user/uclresearchanalysis/data/nyc/pickle/friends.dat',\n",
      "                     'needcrawl': '/home/ec2-user/uclresearchanalysis/data/nyc/pickle/needcrawl.dat',\n",
      "                     'network': '/home/ec2-user/uclresearchanalysis/data/nyc/pickle/network.dat',\n",
      "                     'tweets': '/home/ec2-user/uclresearchanalysis/data/nyc/pickle/tweets.dat',\n",
      "                     'users': '/home/ec2-user/uclresearchanalysis/data/nyc/pickle/users.dat'},\n",
      "          'result': '/home/ec2-user/uclresearchanalysis/data/nyc/result',\n",
      "          'twitter': '/home/ec2-user/uclresearchanalysis/data/nyc/twitter'},\n",
      " 'save_to_file': 'False',\n",
      " 'timeframe': '1440'}\n"
     ]
    }
   ],
   "source": [
    "import builtins\n",
    "# builtins.uclresearch_topic = 'GIVENCHY'\n",
    "# builtins.uclresearch_topic = 'HAWKING'\n",
    "builtins.uclresearch_topic = 'NYC'\n",
    "# builtins.uclresearch_topic = 'FLORIDA'\n",
    "from configuration import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "from pprint import pprint\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calculate_uniquetweets = config.settings['calculate']['uniquetweets']\n",
    "calculate_uniqueusers = config.settings['calculate']['uniqueusers']\n",
    "calculate_network = config.settings['calculate']['network']\n",
    "calculate_analysis = config.settings['calculate']['analysis']\n",
    "calculate_friends = config.settings['calculate']['friends']\n",
    "\n",
    "file_input_path = config.settings['path']['twitter']\n",
    "dates = config.settings['data']['dates']\n",
    "search_phrases = config.settings['data']['phrases']\n",
    "timeframe = config.settings['timeframe']\n",
    "project_name = config.settings['data']['eventname']\n",
    "starttime = config.settings['data']['starttime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order_and_reindex(df, column):\n",
    "    df = df.sort_values(by=[column])\n",
    "    df = df.set_index(np.arange(len(df.index)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_utc_to_est(time_string):\n",
    "    datetime_object = dt.datetime.strptime(time_string, '%a %b %d %H:%M:%S %z %Y')\n",
    "    return datetime_object.replace(tzinfo=timezone.utc).astimezone(tz=timezone(-timedelta(hours=5)))\n",
    "\n",
    "def get_created_at(tweet):\n",
    "    return convert_utc_to_est(tweet['created_at'])\n",
    "\n",
    "def get_user_created_days(tweet):\n",
    "    delta_time = convert_utc_to_est(tweet['created_at']) - convert_utc_to_est(tweet['user']['created_at'])\n",
    "    return delta_time.days + 1\n",
    "\n",
    "def get_retweet_id(tweet):\n",
    "    if (tweet['text'].split()[0] == 'RT'):\n",
    "        user_name = tweet['text'].split()[1][1:-1]\n",
    "        mentions = tweet['entities']['user_mentions']\n",
    "        for mention in mentions:\n",
    "            if mention['screen_name'] == user_name:\n",
    "                return string_to_int(mention['id'])\n",
    "\n",
    "def get_reply_id(tweet):\n",
    "    return string_to_int(tweet['in_reply_to_user_id_str'])\n",
    "    \n",
    "def get_user_mentions(tweet):\n",
    "    retweet_id = get_retweet_id(tweet)\n",
    "    reply_id = get_reply_id(tweet)  \n",
    "    mentions = []\n",
    "    for mention in tweet['entities']['user_mentions']:\n",
    "        mention_id = string_to_int(mention['id'])\n",
    "        if mention_id != retweet_id and mention_id != reply_id:\n",
    "            mentions.append(mention_id)\n",
    "    return mentions\n",
    "\n",
    "def string_to_int(string):\n",
    "    if string is None:\n",
    "        return None\n",
    "    else:\n",
    "        return int(string)\n",
    "\n",
    "def find_unique_tweets_crawled():\n",
    "    file_path_dict = {\n",
    "        date: ['{}/{}_{}.json'.format(file_input_path, x, date) for x in search_phrases]\n",
    "        for date in dates\n",
    "    }\n",
    "    tweets_crawled_list = []\n",
    "    for date, file_path_list in file_path_dict.items():\n",
    "        for file_path in file_path_list:\n",
    "            if (os.path.isfile(file_path)):\n",
    "                with open(file_path, 'r') as file:\n",
    "                    counter = 0\n",
    "                    for line in file.readlines():\n",
    "                        tweets_crawled_list.append(json.loads(line))\n",
    "                        counter += 1\n",
    "                    print('{}, {}, {}'.format(date, file_path, counter))\n",
    "    \n",
    "    unique_tweets = list({each['id']:each for each in tweets_crawled_list}.values())\n",
    "    start_timestamp = dt.datetime.strptime(starttime, '%b %d %H:%M:%S %z %Y')\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['user'] = list(map(lambda tweet: tweet['user']['screen_name'], unique_tweets))\n",
    "    df['user_statuses_count'] = list(map(lambda tweet: int(tweet['user']['statuses_count']), unique_tweets))\n",
    "    df['user_followers_count'] = list(map(lambda tweet: int(tweet['user']['followers_count']), unique_tweets))\n",
    "    df['user_favourites_count'] = list(map(lambda tweet: int(tweet['user']['favourites_count']), unique_tweets))\n",
    "    df['user_listed_count'] = list(map(lambda tweet: int(tweet['user']['listed_count']), unique_tweets))\n",
    "    df['user_friends_count'] = list(map(lambda tweet: int(tweet['user']['friends_count']), unique_tweets))\n",
    "    df['user_created_days'] = list(map(lambda tweet: get_user_created_days(tweet), unique_tweets))\n",
    "    df['user_id'] = list(map(lambda tweet: string_to_int(tweet['user']['id_str']), unique_tweets))\n",
    "    df['created_at'] = list(map(lambda tweet: get_created_at(tweet), unique_tweets))\n",
    "    df['followers_count'] = list(map(lambda tweet: int(tweet['user']['followers_count']), unique_tweets))\n",
    "    df['friends_count'] = list(map(lambda tweet: int(tweet['user']['friends_count']), unique_tweets))\n",
    "    df['reply_id'] = list(map(lambda tweet: get_reply_id(tweet), unique_tweets))\n",
    "    df['retweet_id'] = list(map(lambda tweet: get_retweet_id(tweet), unique_tweets))\n",
    "    df['at_ids'] = list(map(lambda tweet: get_user_mentions(tweet), unique_tweets))\n",
    "    df['text'] = list(map(lambda tweet: tweet['text'], unique_tweets))\n",
    "    \n",
    "    df = df[df.created_at >= start_timestamp]\n",
    "    \n",
    "    df = order_and_reindex(df, 'created_at')\n",
    "    df['time_lapsed'] = 0\n",
    "    first_tweet_datetime = df.created_at.iloc[0]\n",
    "    for index in tqdm(range(len(df))):\n",
    "        df.loc[index, 'time_lapsed'] = round((df.loc[index, 'created_at'] - first_tweet_datetime).total_seconds() / 60.0, 2)\n",
    "    df = df[df.time_lapsed < float(timeframe)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20explosion_2017-12-10.json, 0\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20bombing_2017-12-10.json, 2\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20attack_2017-12-10.json, 32\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20terror_2017-12-10.json, 10\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20explosion_2017-12-10.json, 7\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20bombing_2017-12-10.json, 2\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20attack_2017-12-10.json, 106\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20terror_2017-12-10.json, 20\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20explosion_2017-12-10.json, 0\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20bombing_2017-12-10.json, 0\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20attack_2017-12-10.json, 31\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20terror_2017-12-10.json, 3\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20explosion_2017-12-10.json, 0\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20bombing_2017-12-10.json, 0\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20attack_2017-12-10.json, 0\n",
      "2017-12-10, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20terror_2017-12-10.json, 0\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20explosion_2017-12-11.json, 22904\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20bombing_2017-12-11.json, 11119\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20attack_2017-12-11.json, 20723\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20terror_2017-12-11.json, 15676\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20explosion_2017-12-11.json, 39376\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20bombing_2017-12-11.json, 4326\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20attack_2017-12-11.json, 14345\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20terror_2017-12-11.json, 7086\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20explosion_2017-12-11.json, 29557\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20bombing_2017-12-11.json, 3307\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20attack_2017-12-11.json, 7019\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20terror_2017-12-11.json, 2480\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20explosion_2017-12-11.json, 62958\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20bombing_2017-12-11.json, 12834\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20attack_2017-12-11.json, 19215\n",
      "2017-12-11, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20terror_2017-12-11.json, 7200\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20explosion_2017-12-12.json, 3329\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20bombing_2017-12-12.json, 8143\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20attack_2017-12-12.json, 16738\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/nyc%20terror_2017-12-12.json, 13878\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20explosion_2017-12-12.json, 4523\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20bombing_2017-12-12.json, 5613\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20attack_2017-12-12.json, 8022\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/new%20york%20terror_2017-12-12.json, 4434\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20explosion_2017-12-12.json, 1865\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20bombing_2017-12-12.json, 800\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20attack_2017-12-12.json, 1840\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/manhattan%20terror_2017-12-12.json, 583\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20explosion_2017-12-12.json, 9562\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20bombing_2017-12-12.json, 11438\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20attack_2017-12-12.json, 6673\n",
      "2017-12-12, /home/ec2-user/uclresearchanalysis/data/nyc/twitter/port%20authority%20terror_2017-12-12.json, 3229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227199/227199 [03:40<00:00, 1030.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              user  user_statuses_count  user_followers_count  \\\n",
      "0          BonniBK                42851                  4850   \n",
      "1       ParkedFree                33056                   882   \n",
      "2        Richieamx                81112                  5468   \n",
      "3         romy_nyc                 5832                   179   \n",
      "4  sajosephine2017                 3949                    35   \n",
      "\n",
      "   user_favourites_count  user_listed_count  user_friends_count  \\\n",
      "0                  53462                 12                4869   \n",
      "1                      0                  6                 771   \n",
      "2                  19623                153                5445   \n",
      "3                   5773                  0                 457   \n",
      "4                   1652                  0                 272   \n",
      "\n",
      "   user_created_days             user_id                created_at  \\\n",
      "0                699          4790374997 2017-12-11 07:06:53-05:00   \n",
      "1                151  885584779151912960 2017-12-11 07:15:47-05:00   \n",
      "2                736          4397021841 2017-12-11 07:16:50-05:00   \n",
      "3               2972            84416376 2017-12-11 07:21:46-05:00   \n",
      "4                 64  917338851546460160 2017-12-11 07:22:37-05:00   \n",
      "\n",
      "   followers_count  friends_count  reply_id   retweet_id        at_ids  \\\n",
      "0             4850           4869       NaN    1049171.0  [4797361833]   \n",
      "1              882            771       NaN          NaN            []   \n",
      "2             5468           5445       NaN  232268199.0            []   \n",
      "3              179            457       NaN  133938408.0            []   \n",
      "4               35            272       NaN    1049171.0  [4797361833]   \n",
      "\n",
      "                                                text  time_lapsed  \n",
      "0  RT @tomwatson: Timely coverage of this brutal ...         0.00  \n",
      "1  NYC Pedestrian Safety Comes Under Scrutiny Aft...         8.90  \n",
      "2  RT @NYGovCuomo: Terror won’t beat New York bec...         9.95  \n",
      "3  RT @krassenstein: Trump is systematically tryi...        14.88  \n",
      "4  RT @tomwatson: Timely coverage of this brutal ...        15.73  \n",
      "Dumping data to path /home/ec2-user/uclresearchanalysis/data/nyc/pickle/tweets.dat\n",
      "('Finished dumping data to path '\n",
      " '/home/ec2-user/uclresearchanalysis/data/nyc/pickle/tweets.dat')\n",
      "Loading data file from path /home/ec2-user/uclresearchanalysis/data/nyc/pickle/tweets.dat\n",
      "'Loaded 201451 entires'\n"
     ]
    }
   ],
   "source": [
    "# if calculate_uniquetweets:\n",
    "unique_tweets = find_unique_tweets_crawled()\n",
    "print(unique_tweets.head())\n",
    "config.dump_tweets_dataframe(unique_tweets)\n",
    "unique_tweets = config.load_tweets_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_tweets['normalized_user_statuses_count'] = np.divide(unique_tweets.user_statuses_count, unique_tweets.user_created_days)\n",
    "unique_tweets['normalized_user_followers_count'] = np.divide(unique_tweets.user_followers_count, unique_tweets.user_created_days)\n",
    "unique_tweets['normalized_user_favourites_count'] = np.divide(unique_tweets.user_favourites_count, unique_tweets.user_created_days)\n",
    "unique_tweets['normalized_user_listed_count'] = np.divide(unique_tweets.user_listed_count, unique_tweets.user_created_days)\n",
    "unique_tweets['normalized_user_friends_count'] = np.divide(unique_tweets.user_friends_count, unique_tweets.user_created_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Friends, and add new crawl relationships if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from path /home/ec2-user/uclresearchanalysis/data/nyc/pickle/friends.dat\n",
      "'Loaded 3783 entires'\n",
      "Loading data file from path /home/ec2-user/uclresearchanalysis/other/newcrawl.dat\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d9437568d055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalculate_friends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmerge_new_friends_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfriends_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_friends_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d9437568d055>\u001b[0m in \u001b[0;36mmerge_new_friends_dictionary\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmerge_new_friends_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfriends_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_friends_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnewcrawl_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_newcrawl_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdump_friends_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfriends_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnewcrawl_dictionary\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uclresearchanalysis/configuration/config.py\u001b[0m in \u001b[0;36mload_newcrawl_dictionary\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_newcrawl_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'newcrawl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdump_newcrawl_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/uclresearchanalysis/configuration/config.py\u001b[0m in \u001b[0;36mload_pickle_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded {} entires'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "def merge_new_friends_dictionary():\n",
    "    friends_dictionary = config.load_friends_dictionary()\n",
    "    newcrawl_dictionary = config.load_newcrawl_dictionary()\n",
    "    dump_friends_dictionary({**friends_dictionary, **newcrawl_dictionary})\n",
    "\n",
    "if calculate_friends:\n",
    "    merge_new_friends_dictionary()\n",
    "    \n",
    "friends_dictionary = config.load_friends_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_by_user_name(df, user_name):\n",
    "    user = df[df.user == user_name]\n",
    "    return user.iloc[0]\n",
    "\n",
    "def find_by_user_id(df, user_id):\n",
    "    user = df[df.user_id == user_id]\n",
    "    return user.iloc[0]\n",
    "\n",
    "def find_index_by_user_id(df, user_id):\n",
    "    return df.user_id[df.user_id == user_id].index.tolist()[0]\n",
    "\n",
    "def find_root_and_generation(df, index):\n",
    "    row = df.iloc[index]\n",
    "    time_lapsed = row.time_lapsed\n",
    "    source_index = row.source_index\n",
    "    generation = int(0)\n",
    "    while source_index is not None:\n",
    "        index = source_index\n",
    "        row = df.iloc[index]\n",
    "        source_index = row.source_index\n",
    "        generation += 1\n",
    "    root_time = row.time_lapsed\n",
    "    return (index, generation, time_lapsed-root_time)\n",
    "\n",
    "def find_unique_users():\n",
    "    df = unique_tweets.copy()\n",
    "    df = df.drop_duplicates(subset = ['user_id'])\n",
    "    columns = [\n",
    "        'user', 'user_id', 'time_lapsed', 'followers_count', 'friends_count',\n",
    "        'user_created_days','user_statuses_count','user_listed_count','user_favourites_count',\n",
    "        'normalized_user_statuses_count', 'normalized_user_followers_count',\n",
    "        'normalized_user_favourites_count', 'normalized_user_listed_count', 'normalized_user_friends_count'\n",
    "    ]\n",
    "    \n",
    "    df = df.loc[:,columns]\n",
    "    df['mention_and_reply'] = [[] for _ in range(len(df))]\n",
    "    df['source_candidates'] = [[] for _ in range(len(df))]\n",
    "    df['source_index'] = [None for _ in range(len(df))]\n",
    "    df['seed_index'] = [None for _ in range(len(df))]\n",
    "    df['generation'] = [None for _ in range(len(df))]\n",
    "    df['time_since_seed'] = [None for _ in range(len(df))]\n",
    "    \n",
    "    df = order_and_reindex(df, 'time_lapsed')\n",
    "    \n",
    "    unique_user_id_set = set([int(x) for x in df.user_id])\n",
    "    \n",
    "    for index in tqdm(range(len(unique_tweets))):\n",
    "        user_name = unique_tweets.loc[index, 'user']\n",
    "        user_id = unique_tweets.loc[index, 'user_id']\n",
    "        reply_id = unique_tweets.loc[index, 'reply_id']\n",
    "        retweet_id = unique_tweets.loc[index, 'retweet_id']\n",
    "        at_ids = unique_tweets.loc[index, 'at_ids']\n",
    "        \n",
    "        if reply_id is not None:\n",
    "            if reply_id in unique_user_id_set:\n",
    "                try:\n",
    "                    find_by_user_id(df, user_id).mention_and_reply.append(find_index_by_user_id(df, int(reply_id)))\n",
    "                except:\n",
    "                    pass\n",
    "        if retweet_id is not None:\n",
    "            if retweet_id in unique_user_id_set:\n",
    "                try:\n",
    "                    find_by_user_id(df, user_id).mention_and_reply.append(find_index_by_user_id(df, int(retweet_id)))\n",
    "                except:\n",
    "                    pass\n",
    "        for at_id in at_ids:\n",
    "            if at_id in unique_user_id_set:\n",
    "                try:\n",
    "                    find_by_user_id(df, at_id).mention_and_reply.append(find_index_by_user_id(df, int(user_id)))\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            \n",
    "    friends_not_found_list = []\n",
    "    for index in tqdm(range(len(df))):\n",
    "        user_id = str(df.loc[index, 'user_id'])\n",
    "        try:\n",
    "            friends = (set(friends_dictionary[int(user_id)]) & unique_user_id_set)\n",
    "            friends_indexes = [find_index_by_user_id(df, x) for x in friends]\n",
    "            friends_indexes.extend(df.loc[index, 'mention_and_reply'])\n",
    "            friends_indexes = sorted([x for x in set(friends_indexes)])\n",
    "            df.loc[index, 'source_candidates'].extend(friends_indexes)\n",
    "            if len(friends_indexes) > 0:\n",
    "                if (friends_indexes[0] < index):\n",
    "                    df.loc[index, 'source_index'] = friends_indexes[0]\n",
    "            df.loc[index, 'seed_index'], df.loc[index, 'generation'], df.loc[index, 'time_since_seed'] = find_root_and_generation(df, index)\n",
    "        except KeyError:\n",
    "            friends_not_found_list.append(index)\n",
    "            \n",
    "    print('Could not load friends for {}/{} entries'.format(len(friends_not_found_list), len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_uniqueusers:\n",
    "    unique_users = find_unique_users()\n",
    "    print(unique_users.head())\n",
    "    config.dump_users_dataframe(unique_users)\n",
    "unique_users = config.load_users_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Friends Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verify_friends_dictionary():\n",
    "    friends_dictionary = config.load_friends_dictionary()\n",
    "    crawled_set = set(friends_dictionary.keys())\n",
    "    users_set = set(unique_users.user_id)\n",
    "    need_to_crawl = users_set - crawled_set\n",
    "    config.dump_needcrawl_set(need_to_crawl)\n",
    "    print('Number of users still need to crawl: {}'.format(len(need_to_crawl)))   \n",
    "    \n",
    "    unwanted = set(crawled_set) - set(users_set)\n",
    "    for unwanted_key in unwanted:\n",
    "        del friends_dictionary[unwanted_key]\n",
    "    config.dump_friends_dictionary(friends_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_friends_dictionary()\n",
    "friends_dictionary = config.load_friends_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nx.write_gexf(network, 'givenchy_network.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_all = nx.DiGraph()\n",
    "for index in tqdm(range(len(unique_users))):\n",
    "    network_all.add_node(index,\n",
    "                         user = unique_users.loc[index, 'user'],\n",
    "                         user_id = unique_users.loc[index, 'user_id'],\n",
    "                         time_lapsed = unique_users.loc[index, 'time_lapsed'],\n",
    "                         followers_count = unique_users.loc[index, 'followers_count'],\n",
    "                         friends_count = unique_users.loc[index, 'friends_count'],\n",
    "                         generation = unique_users.loc[index, 'generation'],\n",
    "                         time_since_seed = unique_users.loc[index, 'time_since_seed'],\n",
    "                        )\n",
    "    \n",
    "    \n",
    "    source_index = unique_users.loc[index, 'source_index']\n",
    "    if source_index is not None:\n",
    "        network_all.add_edge(source_index, index)\n",
    "\n",
    "config.dump_networkx_all(network_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_friends = nx.DiGraph()\n",
    "for index in tqdm(range(len(unique_users))):\n",
    "    network_friends.add_node(index,\n",
    "                             user = unique_users.loc[index, 'user'],\n",
    "                             user_id = unique_users.loc[index, 'user_id'],\n",
    "                             time_lapsed = unique_users.loc[index, 'time_lapsed'],\n",
    "                             followers_count = unique_users.loc[index, 'followers_count'],\n",
    "                             friends_count = unique_users.loc[index, 'friends_count'],\n",
    "                             generation = unique_users.loc[index, 'generation'],\n",
    "                             time_since_seed = unique_users.loc[index, 'time_since_seed'],\n",
    "                            )\n",
    "    source_candidates = unique_users.iloc[index].source_candidates\n",
    "    for source_index in source_candidates:\n",
    "        network_friends.add_edge(source_index, index)\n",
    "config.dump_networkx_friends(network_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_potential = nx.DiGraph()\n",
    "for index in tqdm(range(len(unique_users))):\n",
    "    network_potential.add_node(index,\n",
    "                             user = unique_users.loc[index, 'user'],\n",
    "                             user_id = unique_users.loc[index, 'user_id'],\n",
    "                             time_lapsed = unique_users.loc[index, 'time_lapsed'],\n",
    "                             followers_count = unique_users.loc[index, 'followers_count'],\n",
    "                             friends_count = unique_users.loc[index, 'friends_count'],\n",
    "                             generation = unique_users.loc[index, 'generation'],\n",
    "                             time_since_seed = unique_users.loc[index, 'time_since_seed'],\n",
    "                            )\n",
    "    source_candidates = unique_users.iloc[index].source_candidates\n",
    "    for source_index in source_candidates:\n",
    "        if source_index < index:\n",
    "            network_potential.add_edge(source_index, index)\n",
    "config.dump_networkx_potential(network_potential)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
