{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search phrase = FIFA%20Germany\n",
      "search limit (start/stop): 2018-06-17 23:59:59\n",
      "max id (starting point) = 1008499531560996865\n",
      "search limit (start/stop): 2018-06-16 23:59:59\n",
      "since id (ending point) = 1008137143699550209\n",
      "count = 1\n",
      "found 100 tweets\n",
      "2018-06-17 23:57:11\n",
      "count = 2\n",
      "found 100 tweets\n",
      "2018-06-17 22:55:34\n",
      "count = 3\n",
      "found 100 tweets\n",
      "2018-06-17 21:47:30\n",
      "count = 4\n",
      "found 100 tweets\n",
      "2018-06-17 20:56:01\n",
      "count = 5\n",
      "found 100 tweets\n",
      "2018-06-17 20:29:18\n",
      "count = 6\n",
      "found 100 tweets\n",
      "2018-06-17 20:02:59\n",
      "count = 7\n",
      "found 100 tweets\n",
      "2018-06-17 19:33:54\n",
      "count = 8\n",
      "found 100 tweets\n",
      "2018-06-17 19:11:17\n",
      "count = 9\n",
      "found 100 tweets\n",
      "2018-06-17 18:54:04\n",
      "count = 10\n",
      "found 100 tweets\n",
      "2018-06-17 18:43:06\n",
      "count = 11\n",
      "found 100 tweets\n",
      "2018-06-17 18:33:56\n",
      "count = 12\n",
      "found 100 tweets\n",
      "2018-06-17 18:25:03\n",
      "count = 13\n",
      "found 100 tweets\n",
      "2018-06-17 18:15:00\n",
      "count = 14\n",
      "found 100 tweets\n",
      "2018-06-17 18:04:56\n",
      "count = 15\n",
      "found 100 tweets\n",
      "2018-06-17 17:58:38\n",
      "count = 16\n",
      "found 100 tweets\n",
      "2018-06-17 17:51:56\n",
      "count = 17\n",
      "found 100 tweets\n",
      "2018-06-17 17:47:14\n",
      "count = 18\n",
      "found 100 tweets\n",
      "2018-06-17 17:42:44\n",
      "count = 19\n",
      "found 100 tweets\n",
      "2018-06-17 17:38:25\n",
      "count = 20\n",
      "found 100 tweets\n",
      "2018-06-17 17:35:10\n",
      "count = 21\n",
      "found 100 tweets\n",
      "2018-06-17 17:31:36\n",
      "count = 22\n",
      "found 100 tweets\n",
      "2018-06-17 17:28:34\n",
      "count = 23\n",
      "found 100 tweets\n",
      "2018-06-17 17:25:57\n",
      "count = 24\n",
      "found 100 tweets\n",
      "2018-06-17 17:23:30\n",
      "count = 25\n",
      "found 100 tweets\n",
      "2018-06-17 17:20:46\n",
      "count = 26\n",
      "found 100 tweets\n",
      "2018-06-17 17:18:25\n",
      "count = 27\n",
      "found 100 tweets\n",
      "2018-06-17 17:16:25\n",
      "count = 28\n",
      "found 100 tweets\n",
      "2018-06-17 17:14:17\n",
      "count = 29\n",
      "found 100 tweets\n",
      "2018-06-17 17:12:42\n",
      "count = 30\n",
      "found 100 tweets\n",
      "2018-06-17 17:11:34\n",
      "count = 31\n",
      "found 100 tweets\n",
      "2018-06-17 17:10:18\n",
      "count = 32\n",
      "found 100 tweets\n",
      "2018-06-17 17:09:02\n",
      "count = 33\n",
      "found 100 tweets\n",
      "2018-06-17 17:07:59\n",
      "count = 34\n",
      "found 100 tweets\n",
      "2018-06-17 17:06:50\n",
      "count = 35\n",
      "found 100 tweets\n",
      "2018-06-17 17:05:51\n",
      "count = 36\n",
      "found 100 tweets\n",
      "2018-06-17 17:04:56\n",
      "count = 37\n",
      "found 100 tweets\n",
      "2018-06-17 17:04:01\n",
      "count = 38\n",
      "found 100 tweets\n",
      "2018-06-17 17:03:17\n",
      "count = 39\n",
      "found 100 tweets\n",
      "2018-06-17 17:02:39\n",
      "count = 40\n",
      "found 100 tweets\n",
      "2018-06-17 17:02:08\n",
      "count = 41\n",
      "found 100 tweets\n",
      "2018-06-17 17:01:36\n",
      "count = 42\n",
      "found 100 tweets\n",
      "2018-06-17 17:01:04\n",
      "count = 43\n",
      "found 100 tweets\n",
      "2018-06-17 17:00:35\n",
      "count = 44\n",
      "found 100 tweets\n",
      "2018-06-17 17:00:15\n",
      "count = 45\n",
      "found 100 tweets\n",
      "2018-06-17 16:59:52\n",
      "count = 46\n",
      "found 100 tweets\n",
      "2018-06-17 16:59:33\n",
      "count = 47\n",
      "found 100 tweets\n",
      "2018-06-17 16:59:11\n",
      "count = 48\n",
      "found 100 tweets\n",
      "2018-06-17 16:58:47\n",
      "count = 49\n",
      "found 100 tweets\n",
      "2018-06-17 16:58:29\n",
      "count = 50\n",
      "found 100 tweets\n",
      "2018-06-17 16:58:14\n",
      "count = 51\n",
      "found 100 tweets\n",
      "2018-06-17 16:57:55\n",
      "count = 52\n",
      "found 100 tweets\n",
      "2018-06-17 16:57:40\n",
      "count = 53\n",
      "found 100 tweets\n",
      "2018-06-17 16:57:22\n",
      "count = 54\n",
      "found 100 tweets\n",
      "2018-06-17 16:57:04\n",
      "count = 55\n",
      "found 100 tweets\n",
      "2018-06-17 16:56:49\n",
      "count = 56\n",
      "found 100 tweets\n",
      "2018-06-17 16:56:36\n",
      "count = 57\n",
      "found 100 tweets\n",
      "2018-06-17 16:56:24\n",
      "count = 58\n",
      "found 100 tweets\n",
      "2018-06-17 16:56:12\n",
      "count = 59\n",
      "found 100 tweets\n",
      "2018-06-17 16:55:59\n",
      "count = 60\n",
      "found 100 tweets\n",
      "2018-06-17 16:55:45\n",
      "count = 61\n",
      "found 100 tweets\n",
      "2018-06-17 16:55:35\n",
      "count = 62\n",
      "found 100 tweets\n",
      "2018-06-17 16:55:22\n",
      "count = 63\n",
      "found 100 tweets\n",
      "2018-06-17 16:55:09\n",
      "count = 64\n",
      "found 100 tweets\n",
      "2018-06-17 16:54:57\n",
      "count = 65\n",
      "found 100 tweets\n",
      "2018-06-17 16:54:42\n",
      "count = 66\n",
      "found 100 tweets\n",
      "2018-06-17 16:54:30\n",
      "count = 67\n",
      "found 100 tweets\n",
      "2018-06-17 16:54:21\n",
      "count = 68\n",
      "found 100 tweets\n",
      "2018-06-17 16:54:10\n",
      "count = 69\n",
      "found 100 tweets\n",
      "2018-06-17 16:54:02\n",
      "count = 70\n",
      "found 100 tweets\n",
      "2018-06-17 16:53:55\n",
      "count = 71\n",
      "found 100 tweets\n",
      "2018-06-17 16:53:46\n",
      "count = 72\n",
      "found 100 tweets\n",
      "2018-06-17 16:53:38\n",
      "count = 73\n",
      "found 100 tweets\n",
      "2018-06-17 16:53:31\n",
      "count = 74\n",
      "found 100 tweets\n",
      "2018-06-17 16:53:25\n",
      "count = 75\n",
      "found 100 tweets\n",
      "2018-06-17 16:53:18\n",
      "count = 76\n",
      "found 100 tweets\n",
      "2018-06-17 16:53:11\n",
      "count = 77\n",
      "found 100 tweets\n",
      "2018-06-17 16:53:05\n",
      "count = 78\n",
      "found 100 tweets\n",
      "2018-06-17 16:52:57\n",
      "count = 79\n",
      "found 100 tweets\n",
      "2018-06-17 16:52:50\n",
      "count = 80\n",
      "found 100 tweets\n",
      "2018-06-17 16:52:42\n",
      "count = 81\n",
      "found 100 tweets\n",
      "2018-06-17 16:52:36\n",
      "count = 82\n",
      "found 100 tweets\n",
      "2018-06-17 16:52:28\n",
      "count = 83\n",
      "found 100 tweets\n",
      "2018-06-17 16:52:21\n",
      "count = 84\n",
      "found 100 tweets\n",
      "2018-06-17 16:52:15\n",
      "count = 85\n",
      "found 100 tweets\n",
      "2018-06-17 16:52:09\n",
      "count = 86\n",
      "found 100 tweets\n",
      "2018-06-17 16:52:01\n",
      "count = 87\n",
      "found 100 tweets\n",
      "2018-06-17 16:51:53\n",
      "count = 88\n",
      "found 100 tweets\n",
      "2018-06-17 16:51:45\n",
      "count = 89\n",
      "found 97 tweets\n",
      "found 0 tweets\n",
      "no tweets found\n",
      "2018-06-17 16:51:36\n",
      "count = 90\n",
      "found 92 tweets\n",
      "found 7 tweets\n",
      "found 1 tweets\n",
      "2018-06-17 16:31:05\n",
      "count = 91\n",
      "found 100 tweets\n",
      "2018-06-17 15:47:07\n",
      "count = 92\n",
      "found 100 tweets\n",
      "2018-06-17 15:28:08\n",
      "count = 93\n",
      "found 100 tweets\n",
      "2018-06-17 15:09:13\n",
      "count = 94\n",
      "found 100 tweets\n",
      "2018-06-17 14:58:15\n",
      "count = 95\n",
      "found 97 tweets\n",
      "found 2 tweets\n",
      "found 1 tweets\n",
      "2018-06-17 13:21:07\n",
      "count = 96\n",
      "found 21 tweets\n",
      "found 0 tweets\n",
      "no tweets found\n",
      "2018-06-17 03:08:35\n",
      "count = 97\n",
      "found 0 tweets\n",
      "no tweets found\n",
      "count = 98\n",
      "found 0 tweets\n",
      "no tweets found\n",
      "count = 99\n",
      "found 0 tweets\n",
      "no tweets found\n",
      "count = 100\n",
      "found 0 tweets\n",
      "no tweets found\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Maximum number of empty tweet strings reached - exiting",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Maximum number of empty tweet strings reached - exiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class TwitterApi(object):\n",
    "    def __init__(self, consumer_key, consumer_secret, access_token, access_secret): \n",
    "        self.consumer_key = consumer_key \n",
    "        self.consumer_secret = consumer_secret \n",
    "        self.access_token = access_token\n",
    "        self.access_secret = access_secret\n",
    "\n",
    "    def loadapi(self):\n",
    "        auth = OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "        auth.set_access_token(self.access_token, self.access_secret)\n",
    "        return tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "\n",
    "def load_twitter_api_list():\n",
    "    twitter_api_list = []\n",
    "    # Jimmy Canteen 0\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'ogd44qNt9NHukPsUsnKmn1Wm3', \n",
    "        'g0PMron4C0eOD7NYpSqekzUwPwRafUpYTOKgiAabA5fkpS2a4l', \n",
    "        '936587907007176704-2xAvOIe5u5FTkuAanmWLJRkwKlMPqnD',\n",
    "        'DTJQl1JjQdbVErGlTGBI7g7wyHeWt8o0eQZJIb4fewn3J'\n",
    "    ).loadapi())\n",
    "    # BarberDdddavid 1\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'Imk26gt1YacAmZqsBvwXNNZp1', \n",
    "        'ZTVGMr0B6e8PLaFWFtQFbBSta4e0E3POjZO0Ps4uU7XX2jp08c', \n",
    "        '948501581372280832-2Ux6iejRB0Mr7KOyQ2psV5hgN8rCvAW',\n",
    "        'cwCusThG5QGtLoLusElVWdMKtZ8VHvJmngBGjqCkpO77S'\n",
    "    ).loadapi())\n",
    "    # Hu13Steve 2\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'Lqz4r2UGJ3MALpX7TygEWkdvu', \n",
    "        '4irrPlKdKpqdo35PAlqx1g7oTLXL79KGh8ul0Ug3NP7NPnJ5fq', \n",
    "        '948494955756052481-HBHi8eC43VnafcgKj1hfQrkiQgdazld',\n",
    "        '1hfPh52QxAWtku4AqNnjUytGCWyt4AkNllR0RGdNag5wW'\n",
    "    ).loadapi())\n",
    "    # james13_david 3\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'QweR1meYNPziTVMQfnPBw7u7L', \n",
    "        'w2imXsR0TKSqafgZP0LU1spudls7SnCcUqskUUubmvmstbah5S', \n",
    "        '940722476585312257-i94glXYrPQa4LYqhLz75qNfaDHXrFpU',\n",
    "        'xezy1i3SKqkiVVBHy25avg1pybkfuR1CIbUzRGj5mq6Mo'\n",
    "    ).loadapi())\n",
    "    # hui's crawler 4\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'FImHy1gMljimQMQxG6432rATE', \n",
    "        '8oNc5WX3srEophN8oiGrnBeta1iX1QWPM9IDl3scyuzitHZkTp', \n",
    "        '1976839820-bH0EOOIuJQC5yqZRDv5iJo8Gpp3Uerz76OOug99',\n",
    "        'ntdfl0pjZZYVHsLTDK1wKiEsKHS5B9mAdvyCDhREQqDzG'\n",
    "    ).loadapi())\n",
    "    # lzhoudevuk 5 \n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'MP1ydHbs9wSPCvUO0HcJlBd1s', \n",
    "        'yeVQtGsq4pgYrX6tBqMtKcscrGULJE9SCPyP38vFDaAhUtSJiw', \n",
    "        '959980110588899329-A943y5m9XFhivFJXXw0N1uRSdesdYlY',\n",
    "        'hP63SwoueduNQSSe4pwlZNhegjJCXVn4qjeZPfv762KsJ'\n",
    "    ).loadapi())\n",
    "    # lzhoudevcn 6 \n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'cM2Ket4JdIrFCoFJfZR6dKEJW', \n",
    "        'VzewbPMeZcIu8aakQ0cnOmhPnJmHhazFDG9cCSgka01JqT463a', \n",
    "        '959978501125402624-iRPiJiSVy7TDihEf1HsV74Ow72iavWO',\n",
    "        'ZvInw0gQb8xRvMLwoG6PDy18JwM66vhhyFnskeisZBGRe'\n",
    "    ).loadapi())\n",
    "    # lzhoudevau passed 7 \n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'pbFZlAThsyleV4IWazYaG6WH0', \n",
    "        'w01obwo4ECZfl4aIbhEA8q5GnMGTKVXiWvsDq6D8eE4DiwTpps', \n",
    "        '959974918619385856-sGUbwHJQd8YrdoNkJGDTyvpafFjh7wi',\n",
    "        '676MhvwxiPZoUv6f8Upm2fVaZiPUa5jDvCwFHA05HowiE'\n",
    "    ).loadapi())\n",
    "    # ucablz4 passed 8\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'CeIvDs3UbPO4Yj4XK6ZFHzVIU', \n",
    "        'j0eRROwvGFk1CEciSehtUOc2SCVmxzMhWbQd3sUDhztdICPtMW', \n",
    "        '930982504596672517-AE7R6i4xaNPUeXME2S4cyeTgSJNBVEv',\n",
    "        'JEmF4hPSTD65lFmYM1Zm3x4I6Xf7kUQ93NvlodJ2jPrQN'\n",
    "    ).loadapi())\n",
    "    # liyi.zhou.17 passed 9\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'AdCVM7NUpICq9RcYnZiM68FLb', \n",
    "        '0hbBfyXK8CBCvl9S6Zu1CkWTlAo1c4rvfsAl8ot4VPehTLpHL4', \n",
    "        '930982019672178689-UpCYlSLDFDiwcr44weBZoDO4CY2npyE',\n",
    "        's3Z5H1cADAMYElfGhYCE27ZQSWNR4EP2n1FcJmcwG80ZB'\n",
    "    ).loadapi())\n",
    "    # lzhou1110 10\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'yom0HoCImxDZobnZzDrJsESke',\n",
    "        'iPDqeGyq40FbovpFUoLdunLnFINEDB5MQuzFFbo0KBoBiM4mk0',\n",
    "        '910787059501150208-vvOcHytvhGncJTtuAF23tywLu5UTSbL',\n",
    "        'lL4vIlZlKMurhhgsZGM11hGh6LG6cUBtaHqbMiLYJTZhj'\n",
    "    ).loadapi())\n",
    "    # tao 11\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'YZD91p9NpWydELrliHkx3FkV2',\n",
    "        'dchHYkpVxp7Uq1Mi5fA94GfCKWFUrQ1VRp0kQJsI2nXXTJjqUk',\n",
    "        '960929372206223360-TgkpvCQWqTdf9Po9DaOJCrOCGHiMGVV',\n",
    "        'pRCYaoy3cKWmzvIZ3rDExlxSBI1SVTVIGLRfvpzFxTaVp'\n",
    "    ).loadapi())\n",
    "    # wu api 12\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        '6hDKNKAEVflvc1QiKWRwEvqIU',\n",
    "        'LAOoZL1qKs4l9VTyWRKOibLj4tm14n6ZziMX9wd9Wo3NJ8ADyC',\n",
    "        '961193810951856129-nGfaxWcrgS4i0gReDmyg68xOZoWiBPU',\n",
    "        'xDwZpIXjrvcUZpqxz39e77XM924Ek9tBQ69aC9JtDPMEC'\n",
    "    ).loadapi())\n",
    "    # peter.liyi.z api 13\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'AFRdbq1jpYv5uHzc2XXilJBMY',\n",
    "        'UvSj6ZnbUG0t3nQASgd7kSHeiCoL14Iadt4f8gI4Y4QhsanovG',\n",
    "        '2166615014-QcdbmCt252E02I2wAMVgXAJMXtE5tgyQpABuAkr',\n",
    "        '5nYYyhO1WenLXLuuQ6Kthlg8xOg7wlcX7zEP1aLg6abHT'\n",
    "    ).loadapi())\n",
    "    # miao api 14\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        'FlmshcUW970JhOuWCrffznTzM',\n",
    "        '33oh92E47FaeQm6GNWvC5axvmN3nRm6IKhKwN7UX7Xys7pfCtA',\n",
    "        '320714154-woEAipLwLvVwkJo2o9i5IYvoSaFGqzV6pN8nTboL',\n",
    "        'M0UVpBAPdGI3yzfXqXd7vhcx4DRttCLV9LsQTjRR7Fpei'\n",
    "    ).loadapi())\n",
    "    # ucl lzhou adlrl account 15\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        '45mvtiqyyq5B1cXi6kPJHq2TO',\n",
    "        'yrCn2qyWiyYNZXgGX0RhHCAUFV0pjg30twa9txtwltX2CYJvRy',\n",
    "        '961358756595535873-obN8fa0WJRG7y5ptgtEbO0btCXWagwx',\n",
    "        'hCKeIKCThs1xmCUsiaMms0jkHXNshVHSVTu1hbUcTa1kz'\n",
    "    ).loadapi())\n",
    "    # ucl lzhou adlrl account ucablz4 16\n",
    "    twitter_api_list.append(TwitterApi(\n",
    "        '75xtkzO360GjmMgLhaSZ1zH0u',\n",
    "        '2QM0T8J3Jwy4aJEuaTJk9GT6PxkcFVpGqgwoqWvmkvgvq7mqdr',\n",
    "        '961359707809157122-EUhutRfRHEBPSeE791pVGzYd2oMMq87',\n",
    "        'WrgrnvyxVNBFNtkDwD4e1sGw2rQNhvbiOU9YaGkgcK1jA'\n",
    "    ).loadapi())\n",
    "    return twitter_api_list\n",
    "\n",
    "def load_twitter_api():\n",
    "    consumer_key = 'yom0HoCImxDZobnZzDrJsESke'\n",
    "    consumer_secret = 'iPDqeGyq40FbovpFUoLdunLnFINEDB5MQuzFFbo0KBoBiM4mk0'\n",
    "    access_token = '910787059501150208-vvOcHytvhGncJTtuAF23tywLu5UTSbL'\n",
    "    access_secret = 'lL4vIlZlKMurhhgsZGM11hGh6LG6cUBtaHqbMiLYJTZhj'\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "\n",
    "def tweet_search(api, query, max_tweets, max_id, since_id, geocode):\n",
    "    searched_tweets = []\n",
    "    while len(searched_tweets) < max_tweets:\n",
    "        remaining_tweets = max_tweets - len(searched_tweets)\n",
    "        try:\n",
    "            new_tweets = api.search(q=query,\n",
    "                                    count=remaining_tweets,\n",
    "                                    since_id=str(since_id),\n",
    "                                    max_id=str(max_id - 1),\n",
    "                                    geocode=geocode)\n",
    "            print('found', len(new_tweets), 'tweets')\n",
    "            if not new_tweets:\n",
    "                print('no tweets found')\n",
    "                break\n",
    "            searched_tweets.extend(new_tweets)\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError:\n",
    "            print('exception raised, waiting 15 minutes')\n",
    "            print('(until:', dt.datetime.now() + dt.timedelta(minutes=15), ')')\n",
    "            time.sleep(15 * 60)\n",
    "            break  # stop the loop\n",
    "    return searched_tweets, max_id\n",
    "\n",
    "\n",
    "def get_tweet_id(api, date='', days_ago=9, query='a'):\n",
    "    \"\"\" The variable 'days_ago' has been initialized to the maximum\n",
    "        amount we are able to search back in time (9).\"\"\"\n",
    "    if date:\n",
    "        # return an ID from the start of the given day\n",
    "        td = date + dt.timedelta(days=1)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        tweet = api.search(q=query, count=1, until=tweet_date)\n",
    "    else:\n",
    "        # return an ID from __ days ago\n",
    "        td = dt.datetime.now() - dt.timedelta(days=days_ago)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        # get list of up to 10 tweets\n",
    "        tweet = api.search(q=query, count=10, until=tweet_date)\n",
    "        print('search limit (start/stop):', tweet[0].created_at)\n",
    "        # return the id of the first tweet in the list\n",
    "        return tweet[0].id\n",
    "\n",
    "\n",
    "def write_tweets(tweets, filename):\n",
    "    \"\"\" Function that appends tweets to a file. \"\"\"\n",
    "    with open(filename, 'a') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet._json, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def get_geo_code_usa():\n",
    "    # this geocode includes nearly all American states (and a large portion of Canada)\n",
    "    # Refer to: http://thoughtfaucet.com/search-twitter-by-location/examples/\n",
    "    return '39.8,-95.583068847656,2500km'\n",
    "\n",
    "\n",
    "def get_file_path(search_query):\n",
    "    json_file_root = 'stephen/' + search_query\n",
    "    return json_file_root\n",
    "\n",
    "\n",
    "def get_file_name(root, max_days_old, min_days_old):\n",
    "    if max_days_old - min_days_old == 1:\n",
    "        d = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "        day = '{0}-{1:0>2}-{2:0>2}'.format(d.year, d.month, d.day)\n",
    "    else:\n",
    "        d1 = dt.datetime.now() - dt.timedelta(days=max_days_old - 1)\n",
    "        d2 = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "        day = '{0}-{1:0>2}-{2:0>2}_to_{3}-{4:0>2}-{5:0>2}'.format(\n",
    "            d1.year, d1.month, d1.day, d2.year, d2.month, d2.day)\n",
    "    json_file = root + '_' + day + '.json'\n",
    "    return json_file\n",
    "\n",
    "\n",
    "def get_max_id(api, json_file, min_days_old):\n",
    "    # set the 'starting point' ID for tweet collection\n",
    "    if os.path.isfile(json_file):\n",
    "        # open the json file and get the latest tweet ID\n",
    "        with open(json_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) > 0:\n",
    "                max_id = json.loads(lines[-1])['id']\n",
    "                print('Searching from the bottom ID in file')\n",
    "                return max_id\n",
    "\n",
    "    # get the ID of a tweet that is min_days_old\n",
    "    if min_days_old == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return get_tweet_id(api, days_ago=(min_days_old - 1))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" This is a script that continuously searches for tweets\n",
    "        that were created over a given number of days. The search\n",
    "        dates and search phrase can be changed below. \"\"\"\n",
    "    nyc_bombing_phrases = [\n",
    "        'nyc%20explosion',\n",
    "        'nyc%20bombing',\n",
    "        'nyc%20attack',\n",
    "        'nyc%20terror',\n",
    "        'new%20york%20explosion',\n",
    "        'new%20york%20bombing',\n",
    "        'new%20york%20attack',\n",
    "        'new%20york%20terror',\n",
    "        'manhattan%20explosion',\n",
    "        'manhattan%20bombing',\n",
    "        'manhattan%20attack',\n",
    "        'manhattan%20terror',\n",
    "        'port%20authority%20explosion',\n",
    "        'port%20authority%20bombing',\n",
    "        'port%20authority%20attack',\n",
    "        'port%20authority%20terror']\n",
    "    \n",
    "    florida_shooting_phrases = [\n",
    "        'florida%20shooting',\n",
    "        'florida%20massacre',\n",
    "        'Stoneman%20Douglas%20High%20School%20shooting',\n",
    "        'Stoneman%20Douglas%20High%20School%20massacre',\n",
    "        'Parkland%20shooting',\n",
    "        'Parkland%20massacre']\n",
    "    \n",
    "    givenchy_death_phrases = [\n",
    "        'givenchy%20passed%20away',\n",
    "        'givenchy%20die',\n",
    "        'givenchy%20death']\n",
    "    \n",
    "    stephen_death_phrases = [\n",
    "        'stephen%20hawking%20passed%20away',\n",
    "        'stephen%20hawking%20die',\n",
    "        'stephen%20hawking%20death']\n",
    "    \n",
    "    kate_spade_death_phrases = [\n",
    "        'kate%20spade%20passed%20away',\n",
    "        'kate%20spade%20die',\n",
    "        'kate%20spade%20death']\n",
    "    \n",
    "    search_phrases = [ 'FIFA%20Germany']\n",
    "\n",
    "\n",
    "    geo_code_usa = get_geo_code_usa()\n",
    "    time_limit = 1.5  # runtime limit in hours\n",
    "    max_tweets = 100  # number of tweets per search (will be iterated over) - maximum is 100\n",
    "    # search limits\n",
    "    # e.g., from 7 to 8 gives current weekday from last week, min_days_old = 0 will search from right now\n",
    "    min_days_old, max_days_old = 1, 2\n",
    "\n",
    "    # loop over search items, creating a new file for each\n",
    "    for search_phrase in search_phrases:\n",
    "        api_list = load_twitter_api_list()\n",
    "\n",
    "        print('Search phrase = {}'.format(search_phrase))\n",
    "        json_file_root = get_file_path(search_phrase)\n",
    "        os.makedirs(os.path.dirname(json_file_root), exist_ok=True)\n",
    "\n",
    "        json_file = get_file_name(json_file_root, max_days_old, min_days_old)\n",
    "\n",
    "        max_id = get_max_id(api_list[0], json_file, min_days_old)\n",
    "        print('max id (starting point) =', max_id)\n",
    "\n",
    "        since_id = get_tweet_id(api_list[0], days_ago=(max_days_old - 1))\n",
    "        print('since id (ending point) =', since_id)\n",
    "\n",
    "        start = dt.datetime.now()\n",
    "        end = start + dt.timedelta(hours=time_limit)\n",
    "        count, exit_count = 0, 0\n",
    "        while dt.datetime.now() < end:\n",
    "            count += 1\n",
    "            print('count =', count)\n",
    "            # collect tweets and update max_id\n",
    "            tweets, max_id = tweet_search(\n",
    "                api_list[count % len(api_list)],\n",
    "                search_phrase,\n",
    "                max_tweets,\n",
    "                max_id=max_id,\n",
    "                since_id=since_id,\n",
    "                geocode=geo_code_usa)\n",
    "            # write tweets to file in JSON format\n",
    "            write_tweets(tweets, json_file)\n",
    "            if tweets:\n",
    "                print(tweets[0].created_at)\n",
    "                exit_count = 0\n",
    "            else:\n",
    "                exit_count += 1\n",
    "                if exit_count == 4:\n",
    "                    if search_phrase == search_phrases[-1]:\n",
    "                        sys.exit('Maximum number of empty tweet strings reached - exiting')\n",
    "                    else:\n",
    "                        print('Maximum number of empty tweet strings reached - breaking')\n",
    "                        break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
